{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Convolutional_Neural_Networks_(CNN)_Representation_in_a_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jacques-ds-ggg/deep-learning-projects/blob/main/Convolutional_Neural_Networks_(CNN)_Representation_in_a_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJL6sl7TRw-8"
      },
      "source": [
        "# Project : Recognizing hardwritten digits\n",
        "_by JacquesDS_\n",
        "\n",
        "In the project at hand, discovering the MNIST handwritten digit recognition problem with the development of a deep learning model in Python using the Keras library, in order to achieve excellent results.\n",
        "\n",
        "> **Why deep learning ?** \n",
        "- Deep Learning is a subfield of machine learning concerned with algorithms inspired by the structure and function of the brain called artificial neural networks.\n",
        "\n",
        "> **Why use keras ?**\n",
        "- Keras is one of the best neural network Frameworks currently around.\n",
        "\n",
        "> **What is the MNIST handwritten digit recognition problem ?**\n",
        "- The MNIST problem is a dataset developed by Yann LeCun, Corinna Cortes and Christopher Burges for evaluating machine learning models on the handwritten digit classification problem.The dataset was constructed from a number of scanned document dataset available from the National Institute of Standards and Technology (NIST). This is where the name for the dataset comes from, as the Modified NIST or MNIST dataset. Images of digits were taken from a variety of scanned documents, normalized in size and centered. This makes it an excellent dataset for evaluating models, allowing the developer to focus on the machine learning with very little data cleaning or preparation required. In this tutorial, we’ll give you a step by step walk-through of how to build a hand-written digit classifier using the MNIST dataset. For someone new to deep learning, this exercise is arguably the “Hello World” equivalent. It consists of 70,000 labeled 28x28 pixel grayscale images of hand-written digits. The dataset is split into 60,000 training images and 10,000 test images. There are 10 classes (one for each of the 10 digits). The task at hand is to train a model using the 60,000 training images and subsequently test its classification accuracy on the 10,000 test images.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K41Eih1Rw--"
      },
      "source": [
        "import numpy as np                                            # import numpy to generate random numbers\n",
        "\n",
        "np.random.seed(10)                                            # for reproducibility"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoHp3ROlRw-_"
      },
      "source": [
        "## Exploring the Data\n",
        "\n",
        "#### Load image data from MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE5fn-73Rw-_",
        "outputId": "c9799a15-5ada-4966-e284-5282038186b2"
      },
      "source": [
        "from keras.datasets import mnist                              # import keras to obtain the dataset within the library\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()      # load_data for splitting data to test & train datasets\n",
        "\n",
        "print('There are %d training images.' % len(X_train))         # Display the test & train datasets lenth\n",
        "print('There are %d testing images.' % len(X_test))\n",
        "print('There are %d testing images.' % len(y_train))\n",
        "print('There are %d testing images.' % len(y_test))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "There are 60000 training images.\n",
            "There are 10000 testing images.\n",
            "There are 60000 testing images.\n",
            "There are 10000 testing images.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "0QNfcm1pRw_A",
        "outputId": "15046d71-6582-4d47-9919-630ac5712272"
      },
      "source": [
        "import matplotlib.pyplot as plt                               # import matplotlib for Charts\n",
        "\n",
        "plt.subplot(231)                                              # plot some random images in (RGB) as well as gray scale format\n",
        "plt.imshow(X_train[13])\n",
        "plt.subplot(232)\n",
        "plt.imshow(X_train[55])\n",
        "plt.subplot(233)\n",
        "plt.imshow(X_train[546])\n",
        "plt.subplot(234)\n",
        "plt.imshow(X_train[489], cmap=plt.get_cmap('gray'))           \n",
        "plt.subplot(235)\n",
        "plt.imshow(X_train[30023], cmap=plt.get_cmap('gray'))\n",
        "plt.subplot(236)\n",
        "plt.imshow(X_train[24670], cmap=plt.get_cmap('gray'))\n",
        "plt.show()                                                     # show the plot"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NuBV9eDBSuVp",
        "outputId": "5d3750d0-150e-47e1-9828-e4e6fa5f9d09"
      },
      "source": [
        "print ('Shape of training dataset:')                          # printing shape of training dataset\n",
        "print (X_train.shape)\n",
        "print ('Shape of testing dataset:')                           # printing shape of testing dataset\n",
        "print (X_test.shape)                                          # Shape required to determine hat type of preprocessing on the dimentions needs to be done"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of training dataset:\n",
            "(60000, 28, 28)\n",
            "Shape of testing dataset:\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRwbaIhiTlXm"
      },
      "source": [
        "## **Preprocessing Data**\n",
        "\n",
        "### Reshaping Dimesions\n",
        "Now we will preprocess the inputs for keras. Our input is structured as a 3d array in the form of (60000, 28, 28), this means the height and depth of image is 28x28 and 60000 is number of images in the dataset. But input in keras requires 4d array. We must add a dimension for the depth of image. A full-color image with all 3 RGB channels will have a depth of 3, but MNIST images only have a depth of 1 and we can add this dimension easily using the **reshape()** function on the NumPy array."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XsnBt-zTd_1",
        "outputId": "b5659660-c6c8-4021-fe5a-12c6211a59f8"
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 1, 28, 28)        # Reshape input data\n",
        "X_test = X_test.reshape(X_test.shape[0], 1, 28, 28)\n",
        "print('Dimensions after reshaping:')                          # Printing the new dimensions\n",
        "print(X_train.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions after reshaping:\n",
            "(60000, 1, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lxZclpNzUOWS"
      },
      "source": [
        "### Normalize the Pixel values\n",
        "Each of the pixels that represents an image stored inside a computer has a pixel value which describes how bright that pixel is, and/or what color it should be. In the simplest case of binary images, the pixel value is a 1-bit number indicating either foreground or background. For a grayscale images, the pixel value is a single number that represents the brightness of the pixel. The most common  pixel format is the byte image, where this number is stored as an 8-bit integer giving a range of possible values from 0 to 255. Typically zero is taken to be black, and 255 is taken to be white. Values in between make up the different shades of gray. It is almost always a good idea to perform some scaling of input values when using neural network models. Because the scale is well known and well behaved, we can very quickly normalize the pixel values to the range 0 and 1 by dividing each value by the maximum of 255 and this will normalize our data values to the range [0, 1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tv8L2b3T9zT"
      },
      "source": [
        "X_train = X_train.astype('float32')                           # convert data type to float32\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train/255                                         # Normalizing pixel values\n",
        "X_test = X_test/255"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6MjYMziUgzb"
      },
      "source": [
        "### Preprocess class labels for Keras\n",
        "This is a multi-class classification problem. The output variable is an integer from 0 to 9. Currently our labels is an array containing 10 classes and we need the labels to be in 10 distinct classes. We can fix this easily by using one hot encoding. There is a built-in **np_utils.to_categorical()** helper function in Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DieQ_jA-Uc7s"
      },
      "source": [
        "from keras.utils import np_utils                              # import the library needed for one hot encoding\n",
        "Y_train = np_utils.to_categorical(y_train, 10)                # One Hot Encode labels\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxIn4OiOUtPX"
      },
      "source": [
        "## **Define model architecture**\n",
        "\n",
        "The Keras layers module provides a high-level API that makes it easy to construct a neural network. It provides methods that facilitate the creation of dense (fully connected) layers and convolutional layers, adding activation functions, and applying dropout regularization. Here, we will learn how to use layers to build a convolutional neural network model to recognize the handwritten digits in the MNIST data set.\n",
        "### **About CNN**\n",
        "Convolutional neural networks (CNNs) are the current state-of-the-art model architecture for image classification tasks. CNNs apply a series of filters to the raw pixel data of an image to extract and learn higher-level features, which the model can then use for classification. Our aim of introducing more and more layers is increasing the depth of image and decreasing the height and depth of the image. More the depth, more patterns the model will learn within the image. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYk6DHeaUqhS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "69a34003-d9d1-49d4-db58-eca57b3d5318"
      },
      "source": [
        "from keras.models import Sequential                                                                     # import necessary libraries for defining model architecture\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import Flatten\n",
        "from keras.layers.convolutional import Convolution2D\n",
        "from keras.layers.convolutional import MaxPooling2D\n",
        "from keras import backend as K\n",
        "K.set_image_data_format('channels_first')\n",
        "\n",
        "model = Sequential()                                                                                    # Define model architecture\n",
        " \n",
        "model.add(Convolution2D(filters = 32, kernel_size = 3, activation='relu', input_shape=(1,28,28)))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Convolution2D(filters = 64, kernel_size = 3, activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 32, 26, 26)        320       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 32, 13, 13)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 64, 11, 11)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 64, 5, 5)          0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 64, 5, 5)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               204928    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 225,034\n",
            "Trainable params: 225,034\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pn7Io-HIVCgN"
      },
      "source": [
        "## Compile and Fit the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym4Fh6h0VAHJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4e6b56-6a23-48b0-d9df-3f42196b3846"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])        # Compile model\n",
        "model.fit(X_train, Y_train, batch_size=200, epochs=2, verbose=1)                            # Fit model on training data"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/2\n",
            "300/300 [==============================] - 33s 11ms/step - loss: 0.7827 - accuracy: 0.7416\n",
            "Epoch 2/2\n",
            "300/300 [==============================] - 3s 11ms/step - loss: 0.1291 - accuracy: 0.9615\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd9d08151d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hgc2RhjVVHHQ"
      },
      "source": [
        "## Evaluate model on test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mw_A2jBrVJg5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdaaf524-6a44-4677-d21f-d5df109cba03"
      },
      "source": [
        "from keras.layers import UpSampling2D\n",
        "\n",
        "score = model.evaluate(X_test, Y_test, verbose=0)                                           # Evaluate model on test data\n",
        "print ('The error rate and Accuracy on test set is as follows : ')\n",
        "print (score)\n",
        "print(UpSampling2D(size=(2, 2), data_format=None))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The error rate and Accuracy on test set is as follows : \n",
            "[0.04678431525826454, 0.9857000112533569]\n",
            "<tensorflow.python.keras.layers.convolutional.UpSampling2D object at 0x7fd9c03f2410>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Iwd7xVUkPTn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}